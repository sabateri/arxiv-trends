{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd354fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import year, col\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93d418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9044cb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8061bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/06/03 13:31:29 WARN Utils: Your hostname, jordi resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/06/03 13:31:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/03 13:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('pyspark-run-with-gcp-bucket') \\\n",
    "    .config(\"spark.jars\", \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GCS credentials if necessary\n",
    "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", \"../keys/arxiv-trends-key.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9eb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203bcef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"gs://test-arxiv-bucket-111865037319658112231/arxiv_data/arxiv_hep-ex_papers_2012_2018.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2545b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/02 18:21:35 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+\n",
      "|               title|             summary|submission_date|                  id|              author|primary_category|          categories|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+\n",
      "|Search for Extra ...|A possible soluti...|     2010-01-29|http://arxiv.org/...|  [Leonardo Benucci]| physics.ins-det|[physics.ins-det,...|\n",
      "|K* resonance effe...|Charged and neutr...|     2010-01-29|http://arxiv.org/...|[O. Leitner, J. -...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|Sivers and Boer-M...|Results for the n...|     2010-01-29|http://arxiv.org/...|[B. Pasquini, F. ...|          hep-ph|[hep-ph, hep-ex, ...|\n",
      "|Triggering collec...|Collective flavor...|     2010-01-29|http://arxiv.org/...|[Basudeb Dasgupta...|          hep-ph|[hep-ph, astro-ph...|\n",
      "|Measurement of Bc...|The $B_c^\\pm$ mas...|     2010-01-29|http://arxiv.org/...|           [Jibo He]|          hep-ex|            [hep-ex]|\n",
      "|Branching fractio...|Using a sample of...|     2010-01-29|http://arxiv.org/...|[The BESIII Colla...|          hep-ex|            [hep-ex]|\n",
      "|Observation of a ...|The decay channel...|     2010-01-29|http://arxiv.org/...|[The BESIII Colla...|          hep-ex|            [hep-ex]|\n",
      "|Effect on Higgs B...|In seesaw models ...|     2010-01-28|http://arxiv.org/...|[Jyong-Hao Chen, ...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|Further study of ...|This paper update...|     2010-01-28|http://arxiv.org/...|[F. Dufour, T. Ka...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|NNLL resummation ...|I present the nex...|     2010-01-27|http://arxiv.org/...|[Nikolaos Kidonakis]|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|Neutrino oscillat...|A consistent desc...|     2010-01-27|http://arxiv.org/...|[Evgeny Kh. Akhme...|          hep-ph|[hep-ph, astro-ph...|\n",
      "|Flavour physics a...|The Tevatron heav...|     2010-01-27|http://arxiv.org/...|    [Giovanni Punzi]|          hep-ex|            [hep-ex]|\n",
      "|Fermi LAT Search ...|Dark matter (DM) ...|     2010-01-27|http://arxiv.org/...|[The Fermi LAT Co...|     astro-ph.HE|[astro-ph.HE, ast...|\n",
      "|Feasibility Studi...|PANDA, the detect...|     2010-01-26|http://arxiv.org/...|         [A. Biegun]| physics.comp-ph|[physics.comp-ph,...|\n",
      "|Updated global fi...|We present an up-...|     2010-01-26|http://arxiv.org/...|[M. C. Gonzalez-G...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|The Spin-dependen...|The inclusive dou...|     2010-01-26|http://arxiv.org/...|[The COMPASS Coll...|          hep-ex|            [hep-ex]|\n",
      "|EMC studies using...|The Anti-Proton A...|     2010-01-26|http://arxiv.org/...| [Aleksandra Biegun]| physics.comp-ph|[physics.comp-ph,...|\n",
      "|Search for $B^0 \\...|We report a searc...|     2010-01-26|http://arxiv.org/...|     [C. -C. Chiang]|          hep-ex|            [hep-ex]|\n",
      "|Search for single...|We report a searc...|     2010-01-26|http://arxiv.org/...|[CDF Collaboratio...|          hep-ex|            [hep-ex]|\n",
      "|The LHC Phenomeno...|We investigate in...|     2010-01-25|http://arxiv.org/...|[Can Kilic, Takem...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c2482",
   "metadata": {},
   "source": [
    "### Schema handling\n",
    "Let's make sure the columns have the correct types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec0ea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('title', StringType(), True), StructField('summary', StringType(), True), StructField('submission_date', DateType(), True), StructField('id', StringType(), True), StructField('author', ArrayType(StringType(), True), True), StructField('primary_category', StringType(), True), StructField('categories', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d759b",
   "metadata": {},
   "source": [
    "Take the first columns to get the types from it, and use pandas to create the spark df from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1652ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(1000).toPandas().to_csv(\"head.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9856bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a3c45c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               object\n",
       "summary             object\n",
       "submission_date     object\n",
       "id                  object\n",
       "author              object\n",
       "primary_category    object\n",
       "categories          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47e0256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('title', StringType(), True), StructField('summary', StringType(), True), StructField('submission_date', StringType(), True), StructField('id', StringType(), True), StructField('author', StringType(), True), StructField('primary_category', StringType(), True), StructField('categories', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895cb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('title', types.StringType(), True),\n",
    "    types.StructField('summary', types.StringType(), True),\n",
    "    types.StructField('submission_date', types.DateType(), True),\n",
    "    types.StructField('id', types.StringType(), True),\n",
    "    types.StructField('author', types.ArrayType(types.StringType()), True),\n",
    "    types.StructField('primary_category', types.StringType(), True),\n",
    "    types.StructField('categories',  types.ArrayType(types.StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef0ec577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "!rm head.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdb27e",
   "metadata": {},
   "source": [
    "Set the correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a5ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 18:21:59,913 - INFO - Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sabateri/anaconda3/envs/arxiv-trends/lib/python3.11/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-06-02 18:21:59,916 - INFO - Closing down clientserver connection\n",
      "2025-06-02 18:21:59,917 - INFO - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sabateri/anaconda3/envs/arxiv-trends/lib/python3.11/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sabateri/anaconda3/envs/arxiv-trends/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sabateri/anaconda3/envs/arxiv-trends/lib/python3.11/site-packages/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "2025-06-02 18:21:59,926 - INFO - Closing down clientserver connection\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .parquet(\"gs://test-arxiv-bucket-111865037319658112231/arxiv_data/arxiv_hep-ex_papers_2012_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e665a914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- submission_date: date (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- author: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- primary_category: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be983940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+\n",
      "|               title|             summary|submission_date|                  id|              author|primary_category|          categories|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+\n",
      "|Search for Extra ...|A possible soluti...|     2010-01-29|http://arxiv.org/...|  [Leonardo Benucci]| physics.ins-det|[physics.ins-det,...|\n",
      "|K* resonance effe...|Charged and neutr...|     2010-01-29|http://arxiv.org/...|[O. Leitner, J. -...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|Sivers and Boer-M...|Results for the n...|     2010-01-29|http://arxiv.org/...|[B. Pasquini, F. ...|          hep-ph|[hep-ph, hep-ex, ...|\n",
      "|Triggering collec...|Collective flavor...|     2010-01-29|http://arxiv.org/...|[Basudeb Dasgupta...|          hep-ph|[hep-ph, astro-ph...|\n",
      "|Measurement of Bc...|The $B_c^\\pm$ mas...|     2010-01-29|http://arxiv.org/...|           [Jibo He]|          hep-ex|            [hep-ex]|\n",
      "|Branching fractio...|Using a sample of...|     2010-01-29|http://arxiv.org/...|[The BESIII Colla...|          hep-ex|            [hep-ex]|\n",
      "|Observation of a ...|The decay channel...|     2010-01-29|http://arxiv.org/...|[The BESIII Colla...|          hep-ex|            [hep-ex]|\n",
      "|Effect on Higgs B...|In seesaw models ...|     2010-01-28|http://arxiv.org/...|[Jyong-Hao Chen, ...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|Further study of ...|This paper update...|     2010-01-28|http://arxiv.org/...|[F. Dufour, T. Ka...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|NNLL resummation ...|I present the nex...|     2010-01-27|http://arxiv.org/...|[Nikolaos Kidonakis]|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|Neutrino oscillat...|A consistent desc...|     2010-01-27|http://arxiv.org/...|[Evgeny Kh. Akhme...|          hep-ph|[hep-ph, astro-ph...|\n",
      "|Flavour physics a...|The Tevatron heav...|     2010-01-27|http://arxiv.org/...|    [Giovanni Punzi]|          hep-ex|            [hep-ex]|\n",
      "|Fermi LAT Search ...|Dark matter (DM) ...|     2010-01-27|http://arxiv.org/...|[The Fermi LAT Co...|     astro-ph.HE|[astro-ph.HE, ast...|\n",
      "|Feasibility Studi...|PANDA, the detect...|     2010-01-26|http://arxiv.org/...|         [A. Biegun]| physics.comp-ph|[physics.comp-ph,...|\n",
      "|Updated global fi...|We present an up-...|     2010-01-26|http://arxiv.org/...|[M. C. Gonzalez-G...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "|The Spin-dependen...|The inclusive dou...|     2010-01-26|http://arxiv.org/...|[The COMPASS Coll...|          hep-ex|            [hep-ex]|\n",
      "|EMC studies using...|The Anti-Proton A...|     2010-01-26|http://arxiv.org/...| [Aleksandra Biegun]| physics.comp-ph|[physics.comp-ph,...|\n",
      "|Search for $B^0 \\...|We report a searc...|     2010-01-26|http://arxiv.org/...|     [C. -C. Chiang]|          hep-ex|            [hep-ex]|\n",
      "|Search for single...|We report a searc...|     2010-01-26|http://arxiv.org/...|[CDF Collaboratio...|          hep-ex|            [hep-ex]|\n",
      "|The LHC Phenomeno...|We investigate in...|     2010-01-25|http://arxiv.org/...|[Can Kilic, Takem...|          hep-ph|    [hep-ph, hep-ex]|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52492030",
   "metadata": {},
   "source": [
    "### Define pyspark UDF's to apply to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d215f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UDF\n",
    "def clean_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Clean and preprocess text by tokenizing, removing stopwords, and lemmatizing.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string to clean\n",
    "        \n",
    "    Returns:\n",
    "        List of cleaned and lemmatized words\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        logger.warning(f\"Expected string but got {type(text)}. Converting to string.\")\n",
    "        text = str(text)\n",
    "        \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error tokenizing text: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words \n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abdbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column(df: DataFrame, column: str) -> DataFrame:    \n",
    "    \"\"\"\n",
    "    Clean the specified column and create a new column with the processed text.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing arXiv papers\n",
    "        column: The column to clean ('title' or 'summary')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added cleaned text column\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the specified column is not in the DataFrame\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "    \n",
    "    cleaned_column = f'cleaned_{column}'\n",
    "    \n",
    "    # Define a UDF to apply your clean_text function\n",
    "    clean_text_udf = udf(clean_text, ArrayType(StringType()))\n",
    "    \n",
    "    logger.info(f\"Processing column: {column}\")\n",
    "    df = df.withColumn(cleaned_column, clean_text_udf(col(column)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f34f99c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 18:22:01,727 - INFO - Processing column: title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 18:22:01,839 - INFO - Processing column: summary\n"
     ]
    }
   ],
   "source": [
    "df = process_column(df, 'title')\n",
    "df = process_column(df, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5e64f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(df: DataFrame, column_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate sentiment polarity score for text in the specified column in a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame containing arXiv papers\n",
    "        column_name: Column name to analyze sentiment from\n",
    "\n",
    "    Returns:\n",
    "        Spark DataFrame with added 'sentiment' column\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified column is not in the DataFrame\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    sentiment_column = f'sentiment_{column_name}'\n",
    "\n",
    "    # Define the UDF\n",
    "    def compute_sentiment(text):\n",
    "        if text is None:\n",
    "            return 0.0\n",
    "        try:\n",
    "            return TextBlob(str(text)).sentiment.polarity\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    sentiment_udf = udf(compute_sentiment, FloatType())\n",
    "    \n",
    "    logger.info(f\"Sentiment analysis completed for column: {column_name}\")\n",
    "    df = df.withColumn(sentiment_column, sentiment_udf(col(column_name)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3a87449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 18:22:01,958 - INFO - Sentiment analysis completed for column: title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 18:22:02,006 - INFO - Sentiment analysis completed for column: summary\n"
     ]
    }
   ],
   "source": [
    "df = calculate_sentiment(df, 'title')\n",
    "df = calculate_sentiment(df, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e32a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+---------------+-----------------+\n",
      "|               title|             summary|submission_date|                  id|              author|primary_category|          categories|       cleaned_title|     cleaned_summary|sentiment_title|sentiment_summary|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+---------------+-----------------+\n",
      "|Search for Extra ...|A possible soluti...|     2010-01-29|http://arxiv.org/...|  [Leonardo Benucci]| physics.ins-det|[physics.ins-det,...|[search, extra, d...|[possible, soluti...|           -0.1|     0.0020562771|\n",
      "|K* resonance effe...|Charged and neutr...|     2010-01-29|http://arxiv.org/...|[O. Leitner, J. -...|          hep-ph|    [hep-ph, hep-ex]|[k, resonance, ef...|[charged, neutral...|            0.1|      0.038690478|\n",
      "|Sivers and Boer-M...|Results for the n...|     2010-01-29|http://arxiv.org/...|[B. Pasquini, F. ...|          hep-ph|[hep-ph, hep-ex, ...|[sivers, function...|[result, quark, d...|            0.0|       0.11833333|\n",
      "|Triggering collec...|Collective flavor...|     2010-01-29|http://arxiv.org/...|[Basudeb Dasgupta...|          hep-ph|[hep-ph, astro-ph...|[triggering, coll...|[collective, flav...|            0.0|         -0.08125|\n",
      "|Measurement of Bc...|The $B_c^\\pm$ mas...|     2010-01-29|http://arxiv.org/...|           [Jibo He]|          hep-ex|            [hep-ex]|[measurement, bc,...|[mass, lifetime, ...|            0.0|            -0.05|\n",
      "|Branching fractio...|Using a sample of...|     2010-01-29|http://arxiv.org/...|[The BESIII Colla...|          hep-ex|            [hep-ex]|[branching, fract...|[using, sample, d...|            0.0|     -0.083333336|\n",
      "|Observation of a ...|The decay channel...|     2010-01-29|http://arxiv.org/...|[The BESIII Colla...|          hep-ex|            [hep-ex]|[observation, ppb...|[decay, channel, ...|            0.0|       0.22083333|\n",
      "|Effect on Higgs B...|In seesaw models ...|     2010-01-28|http://arxiv.org/...|[Jyong-Hao Chen, ...|          hep-ph|    [hep-ph, hep-ex]|[effect, higgs, b...|[seesaw, model, o...|     0.21428572|       0.13739178|\n",
      "|Further study of ...|This paper update...|     2010-01-28|http://arxiv.org/...|[F. Dufour, T. Ka...|          hep-ph|    [hep-ph, hep-ex]|[study, neutrino,...|[paper, update, i...|            0.0|      0.083333336|\n",
      "|NNLL resummation ...|I present the nex...|     2010-01-27|http://arxiv.org/...|[Nikolaos Kidonakis]|          hep-ph|    [hep-ph, hep-ex]|[nnll, resummatio...|[present, nnll, r...|     0.21428572|       0.14862637|\n",
      "|Neutrino oscillat...|A consistent desc...|     2010-01-27|http://arxiv.org/...|[Evgeny Kh. Akhme...|          hep-ph|[hep-ph, astro-ph...|[neutrino, oscill...|[consistent, desc...|            0.0|       0.26041666|\n",
      "|Flavour physics a...|The Tevatron heav...|     2010-01-27|http://arxiv.org/...|    [Giovanni Punzi]|          hep-ex|            [hep-ex]|[flavour, physic,...|[tevatron, heavy,...|            0.0|      0.046780303|\n",
      "|Fermi LAT Search ...|Dark matter (DM) ...|     2010-01-27|http://arxiv.org/...|[The Fermi LAT Co...|     astro-ph.HE|[astro-ph.HE, ast...|[fermi, lat, sear...|[dark, matter, dm...|          -0.15|       0.10918368|\n",
      "|Feasibility Studi...|PANDA, the detect...|     2010-01-26|http://arxiv.org/...|         [A. Biegun]| physics.comp-ph|[physics.comp-ph,...|[feasibility, stu...|[panda, detector,...|            0.7|       0.18888889|\n",
      "|Updated global fi...|We present an up-...|     2010-01-26|http://arxiv.org/...|[M. C. Gonzalez-G...|          hep-ph|    [hep-ph, hep-ex]|[updated, global,...|[present, global,...|            0.2|        0.0882353|\n",
      "|The Spin-dependen...|The inclusive dou...|     2010-01-26|http://arxiv.org/...|[The COMPASS Coll...|          hep-ex|            [hep-ex]|[structure, funct...|[inclusive, asymm...|            0.0|       0.25566378|\n",
      "|EMC studies using...|The Anti-Proton A...|     2010-01-26|http://arxiv.org/...| [Aleksandra Biegun]| physics.comp-ph|[physics.comp-ph,...|[emc, study, usin...|[annihilation, da...|            0.0|             0.22|\n",
      "|Search for $B^0 \\...|We report a searc...|     2010-01-26|http://arxiv.org/...|     [C. -C. Chiang]|          hep-ex|            [hep-ex]|  [search, k, decay]|[report, search, ...|            0.0|         -0.03125|\n",
      "|Search for single...|We report a searc...|     2010-01-26|http://arxiv.org/...|[CDF Collaboratio...|          hep-ex|            [hep-ex]|[search, single, ...|[report, search, ...|     0.07619048|         0.115625|\n",
      "|The LHC Phenomeno...|We investigate in...|     2010-01-25|http://arxiv.org/...|[Can Kilic, Takem...|          hep-ph|    [hep-ph, hep-ex]|[lhc, phenomenolo...|[investigate, det...|            0.0|       0.08177391|\n",
      "+--------------------+--------------------+---------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+---------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7520172",
   "metadata": {},
   "source": [
    "Save the pyspark df into google cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84ebf0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save all the output in one parquet file\n",
    "# Coalesce the DataFrame to a single partition\n",
    "df = df.coalesce(1)\n",
    "output_path = \"gs://test-arxiv-bucket-111865037319658112231/arxiv_data/processed/arxiv_hep-ex_papers_2012_2018_cleaned.parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1195c84",
   "metadata": {},
   "source": [
    "We can also partition it in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b433e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write partitioned Parquet\n",
    "\n",
    "# Add a year column\n",
    "# df = df.withColumn(\"year\", year(col(\"submission_date\")))\n",
    "\n",
    "# df.write \\\n",
    "#     .partitionBy(\"year\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"gs://test-arxiv-bucket-111865037319658112231/arxiv_data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26c0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831df01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-trends",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
